{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b98795-0048-49d9-82f9-9ee3421a7fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Transaction ID: string (nullable = true)\n |-- Date: string (nullable = true)\n |-- Customer ID: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: string (nullable = true)\n |-- Product Category: string (nullable = true)\n |-- Quantity: string (nullable = true)\n |-- Price per Unit: string (nullable = true)\n |-- Total Amount: string (nullable = true)\n |-- Year: string (nullable = true)\n |-- Month: string (nullable = true)\n\nNo se encontraron nuevos datos para la fecha 2024-11-25.\n"
     ]
    }
   ],
   "source": [
    "# ACTULZAIR DATOS DE TABLA DELTA EN UN PERIODO DE TIEMPO DISPONIBLE\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "# Obtener la fecha actual para procesar (formato: yyyy-MM-dd)\n",
    "fecha_procesada = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Ruta de destino en Azure Blob Storage\n",
    "ruta_destino = \"abfss://data@storagepredictsell.blob.core.windows.net/cleaned_retail\"\n",
    "\n",
    "# Tabla Delta donde se almacenan los datos\n",
    "tabla_delta = \"default.cleaned_retail\"\n",
    "\n",
    "try:\n",
    "    # Cargar los nuevos datos \n",
    "    df_nuevos_datos = spark.read.format(\"csv\").option(\"header\", True).load(\"dbfs:/mnt/storagepredictsell/data/cleaned_retail.csv\")\n",
    "\n",
    "    # Validar esquema\n",
    "    df_nuevos_datos.printSchema()\n",
    "\n",
    "    # Filtrar solo los datos de la fecha procesada en el nuevo DataFrame\n",
    "    df_nuevos_datos = df_nuevos_datos.filter(col(\"Date\") == fecha_procesada).dropDuplicates()\n",
    "\n",
    "    # Validar que existen datos para la fecha procesada\n",
    "    if df_nuevos_datos.count() > 0:\n",
    "        # Usar Delta Merge para actualizar eficientemente\n",
    "        tabla_delta_obj = DeltaTable.forName(spark, tabla_delta)\n",
    "        tabla_delta_obj.alias(\"target\").merge(\n",
    "            df_nuevos_datos.alias(\"source\"),\n",
    "            \"target.Date = source.Date\"\n",
    "        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "        # Guardar la tabla Delta en el contenedor de Azure\n",
    "        df_nuevos_datos.write.format(\"delta\").mode(\"overwrite\").save(ruta_destino)\n",
    "\n",
    "        # Confirmar que los datos han sido actualizados correctamente\n",
    "        print(f\"Datos actualizados para la fecha {fecha_procesada}:\")\n",
    "        display(spark.sql(f\"SELECT * FROM {tabla_delta} WHERE Date = '{fecha_procesada}'\"))\n",
    "    else:\n",
    "        print(f\"No se encontraron nuevos datos para la fecha {fecha_procesada}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la actualizaci√≥n: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Automatizacion_tabladelta",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
